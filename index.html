<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Korean Grammar Coach · Voice Agent</title>

  <!-- Correct Tailwind CDN -->
  <script src="https://cdn.tailwindcss.com"></script>
  <style>body{background:#f7f7f8}</style>
</head>
<body>
  <div class="w-full max-w-md mx-auto p-4">
    <div class="rounded-3xl border border-gray-200 shadow-sm p-5 bg-white">
      <div class="flex items-center justify-between mb-4">
        <h1 class="text-lg font-semibold tracking-tight">Korean Grammar Coach</h1>
        <span id="statusChip" class="text-xs px-2 py-1 rounded-full bg-gray-50 text-gray-600">idle</span>
      </div>

      <div class="flex flex-col items-center gap-3 py-6">
        <!-- Timer -->
        <div id="timerText" class="text-4xl tabular-nums font-mono">0:00.000</div>
        <div id="helperText" class="text-xs text-gray-500">timer will start when you speak</div>

        <!-- Controls -->
        <div class="flex gap-3 mt-4">
          <button id="startBtn" class="px-4 py-2 rounded-2xl border border-gray-300 hover:bg-gray-50 active:scale-[.99]">Start</button>
          <button id="stopBtn" style="display:none" class="px-4 py-2 rounded-2xl border border-gray-300 hover:bg-gray-50 active:scale-[.99]">Stop</button>
        </div>

        <!-- Mic visualizer (dot grows with RMS) -->
        <div class="mt-2 h-8 flex items-center justify-center">
          <div id="micDot" aria-hidden class="rounded-full" style="width:12px;height:12px;background:#111;opacity:.8"></div>
        </div>
      </div>

      <div id="warnBox" class="text-sm text-amber-700 bg-amber-50 border border-amber-200 rounded-lg p-3 hidden"></div>
      <div id="errorBox" class="text-sm text-rose-700 bg-rose-50 border border-rose-200 rounded-lg p-3 hidden mt-3"></div>

      <div class="mt-4 text-[11px] text-gray-500 leading-relaxed">
        Public agents can use WebRTC with just an agentId. Private agents need a conversation token from your backend. Site must be served over HTTPS.
      </div>
    </div>

    <div class="text-[11px] text-gray-400 mt-3 text-center">
      Built for @askcalvinoppa · Minimal container · Timer starts on speech
    </div>
  </div>

  <script type="module">
    // =====================
    // CONFIG
    // =====================
    const AGENT_ID = "agent_5001k5x6hf18fzs8bpph834q4ebg"; // TODO: replace with your public agent ID
    const USE_WEBSOCKET = false; // leave false for WebRTC; if you flip this later, you’ll need a signed URL / token.

    // Try to import ElevenLabs SDK from CDN; if it fails, we run in Test Mode.
    let Eleven = null;
    let Conversation = null;
    try {
      Eleven = await import("https://cdn.jsdelivr.net/npm/@elevenlabs/client/dist/elevenlabs.min.mjs");
      Conversation = Eleven?.Conversation;
    } catch (e) {
      // Fall through to Test Mode
    }

    // =====================
    // UI elements
    // =====================
    const statusChip = document.getElementById("statusChip");
    const timerText  = document.getElementById("timerText");
    const helperText = document.getElementById("helperText");
    const startBtn   = document.getElementById("startBtn");
    const stopBtn    = document.getElementById("stopBtn");
    const micDot     = document.getElementById("micDot");
    const warnBox    = document.getElementById("warnBox");
    const errorBox   = document.getElementById("errorBox");

    // =====================
    // State
    // =====================
    let connected = false;
    let speaking  = false;
    let status    = "idle";
    let elapsed   = 0;

    let speechStartedAt = null;
    let rafId = null;

    let mediaStream = null;
    let audioCtx = null;
    let analyser = null;
    let source = null;

    let conversation = null; // ElevenLabs session or null

    // ===== UTIL =====
    function fmt(ms) {
      const mm  = Math.floor(ms / 60000);
      const ss  = Math.floor((ms % 60000) / 1000);
      const ms3 = String(Math.floor(ms % 1000)).padStart(3, "0");
      return `${mm}:${String(ss).padStart(2, "0")}.${ms3}`;
    }

    function setStatus(newStatus) {
      status = newStatus;
      statusChip.textContent = newStatus;
      statusChip.className = "text-xs px-2 py-1 rounded-full";
      if (connected) {
        statusChip.classList.add("bg-emerald-50","text-emerald-700");
      } else if (newStatus === "connecting" || newStatus === "requesting-mic") {
        statusChip.classList.add("bg-amber-50","text-amber-700");
      } else if (newStatus === "error") {
        statusChip.classList.add("bg-rose-50","text-rose-700");
      } else {
        statusChip.classList.add("bg-gray-50","text-gray-600");
      }
    }

    function setConnected(isConnected) {
      connected = isConnected;
      startBtn.style.display = connected ? "none" : "";
      stopBtn.style.display  = connected ? "" : "none";
      setStatus(connected ? "connected" : "idle");
    }

    function setError(msg) {
      if (!msg) {
        errorBox.classList.add("hidden");
        errorBox.textContent = "";
      } else {
        errorBox.classList.remove("hidden");
        errorBox.textContent = msg;
      }
    }

    function setWarn(msg) {
      if (!msg) {
        warnBox.classList.add("hidden");
        warnBox.textContent = "";
      } else {
        warnBox.classList.remove("hidden");
        warnBox.textContent = msg;
      }
    }

    // =====================
    // VAD loop (RMS)
    // =====================
    function startVadLoop() {
      if (!analyser) return;
      const data = new Uint8Array(analyser.fftSize);
      const threshold = 10; // tweak (0..255)

      const loop = () => {
        analyser.getByteTimeDomainData(data);
        let sum = 0;
        for (let i = 0; i < data.length; i++) {
          const centered = data[i] - 128;
          sum += centered * centered;
        }
        const rms = Math.sqrt(sum / data.length);

        if (!speaking && rms > threshold) {
          speaking = true;
          if (!speechStartedAt) speechStartedAt = performance.now();
          helperText.textContent = "listening… timer running";
        }
        if (speaking && speechStartedAt != null) {
          const now = performance.now();
          elapsed = now - speechStartedAt;
          timerText.textContent = fmt(elapsed);
        }

        const radius = 6 + Math.min(18, rms / 3);
        micDot.style.width = `${radius * 2}px`;
        micDot.style.height = `${radius * 2}px`;

        rafId = requestAnimationFrame(loop);
      };
      rafId = requestAnimationFrame(loop);
    }

    function stopVadLoop() {
      if (rafId) cancelAnimationFrame(rafId);
      rafId = null;
    }

    // =====================
    // Audio chain
    // =====================
    async function setupAudioChain() {
      // iOS/Safari: AudioContext must start after user gesture; we’re in click handler, so good.
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaStream = stream;

      const Ctx = window.AudioContext || window.webkitAudioContext;
      audioCtx = new Ctx();

      analyser = audioCtx.createAnalyser();
      analyser.fftSize = 1024;

      source = audioCtx.createMediaStreamSource(stream);
      source.connect(analyser);

      // Resume context if suspended (Chrome autostart policy)
      if (audioCtx.state === "suspended") {
        await audioCtx.resume();
      }

      startVadLoop();
    }

    async function teardownAudioChain() {
      stopVadLoop();

      try { source && source.disconnect(); } catch {}
      try { analyser && analyser.disconnect(); } catch {}

      if (mediaStream) {
        mediaStream.getTracks().forEach(t => t.stop());
        mediaStream = null;
      }
      if (audioCtx) {
        try { await audioCtx.close(); } catch {}
        audioCtx = null;
      }

      speaking = false;
      speechStartedAt = null;
      elapsed = 0;
      timerText.textContent = fmt(0);
      helperText.textContent = "timer will start when you speak";
    }

    // =====================
    // ElevenLabs start/stop with fallback
    // =====================
    async function startElevenLabs() {
      if (!Conversation) throw new Error("ElevenLabs SDK not loaded");
      const opts = USE_WEBSOCKET
        ? { agentId: AGENT_ID, connectionType: "websocket" }
        : { agentId: AGENT_ID, connectionType: "webrtc" };
      return Conversation.startSession(opts);
    }

    // Simple fallback “Test Mode”: speaks one sentence with Web Speech API
    function testModeSpeakOnce(text) {
      try {
        const synth = window.speechSynthesis;
        if (!synth) return;
        const uttr = new SpeechSynthesisUtterance(text);
        synth.cancel();
        synth.speak(uttr);
      } catch {}
    }

    async function handleStart() {
      setError(null);
      setWarn(null);
      setStatus("requesting-mic");

      try {
        await setupAudioChain();
        setStatus("connecting");

        // Try ElevenLabs; if that fails, switch to Test Mode
        let usedTestMode = false;
        try {
          if (Conversation) {
            conversation = await startElevenLabs();
          } else {
            usedTestMode = true;
            setWarn("Running in Test Mode (ElevenLabs SDK not loaded). I’ll still show mic activity and a demo voice line.");
          }
        } catch (e) {
          usedTestMode = true;
          setWarn("Failed to start ElevenLabs session. Running in Test Mode instead.");
        }

        setConnected(true);

        // In Test Mode, play one demo response so the user sees “something happens”
        if (usedTestMode) {
          testModeSpeakOnce("안녕하세요! This is a local demo voice. ElevenLabs is not connected, but your mic and timer are working.");
        }
      } catch (e) {
        console.error(e);
        setError(e?.message || String(e));
        setStatus("error");
        await teardownAudioChain();
        setConnected(false);
      }
    }

    async function handleStop() {
      setStatus("stopping");
      try {
        if (conversation) {
          if (typeof conversation.endSession === "function") {
            await conversation.endSession();
          } else if (typeof conversation.stopSession === "function") {
            await conversation.stopSession();
          }
        }
      } catch {}
      conversation = null;
      await teardownAudioChain();
      setConnected(false);
    }

    // =====================
    // Bind UI
    // =====================
    startBtn.addEventListener("click", handleStart);
    stopBtn.addEventListener("click", handleStop);
  </script>
</body>
</html>
