import React, { useEffect, useMemo, useRef, useState } from "react";

// ⚠️ Install deps:
//   npm i @elevenlabs/client framer-motion
// TailwindCSS is assumed available (but not required). Remove classes if not using Tailwind.

// ================================
// Minimal Voice Agent Container
// - Timer starts when the user actually starts speaking (voice activity detection via Web Audio RMS)
// - Connects to ElevenLabs Agent via JS SDK (WebRTC for simplicity)
// - Plays agent responses and streams mic audio
// - Clean, minimal UI
// ================================

export default function VoiceAgentContainer() {
  const [connected, setConnected] = useState(false);
  const [speaking, setSpeaking] = useState(false); // user speaking
  const [status, setStatus] = useState("idle");
  const [elapsed, setElapsed] = useState(0); // milliseconds
  const [error, setError] = useState<string | null>(null);

  // Timer refs
  const speechStartedRef = useRef<number | null>(null);
  const rafRef = useRef<number | null>(null);

  // Audio + VAD refs
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const audioCtxRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);

  // ElevenLabs conversation session (lazy)
  const conversationRef = useRef<any>(null);

  // ====== CONFIG ======
  // Never hardcode the API key in client-side code. If your agent is PRIVATE,
  // you must mint a conversation token on your server and pass it here.
  // For PUBLIC agents, the agentId alone is sufficient for WebRTC.
  const agentId = (import.meta as any).env?.VITE_ELEVENLABS_AGENT_ID || "agent_5001k5x6hf18fzs8bpph834q4ebg";

  // ====== UTIL: format ms to mm:ss.mmm ======
  const fmt = (ms: number) => {
    const mm = Math.floor(ms / 60000);
    const ss = Math.floor((ms % 60000) / 1000);
    const ms3 = Math.floor(ms % 1000).toString().padStart(3, "0");
    return `${mm}:${ss.toString().padStart(2, "0")}.${ms3}`;
  };

  // ====== VAD loop (simple RMS threshold) ======
  const startVadLoop = () => {
    const analyser = analyserRef.current;
    if (!analyser) return;

    const data = new Uint8Array(analyser.fftSize);
    const threshold = 10; // tweak as needed (0..255). ~10 is gentle.

    const loop = () => {
      analyser.getByteTimeDomainData(data);
      // RMS
      let sum = 0;
      for (let i = 0; i < data.length; i++) {
        const centered = data[i] - 128;
        sum += centered * centered;
      }
      const rms = Math.sqrt(sum / data.length);

      // Start timer on first detected speech burst
      if (!speaking && rms > threshold) {
        setSpeaking(true);
        if (!speechStartedRef.current) {
          speechStartedRef.current = performance.now();
        }
      }

      // If speaking, update the elapsed timer
      if (speaking && speechStartedRef.current != null) {
        const now = performance.now();
        setElapsed(now - speechStartedRef.current);
      }

      rafRef.current = requestAnimationFrame(loop);
    };

    rafRef.current = requestAnimationFrame(loop);
  };

  const stopVadLoop = () => {
    if (rafRef.current) cancelAnimationFrame(rafRef.current);
    rafRef.current = null;
  };

  // ====== Setup audio input + analyser ======
  const setupAudioChain = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    mediaStreamRef.current = stream;

    const ctx = new (window.AudioContext || (window as any).webkitAudioContext)();
    audioCtxRef.current = ctx;

    const analyser = ctx.createAnalyser();
    analyser.fftSize = 1024; // fine for VAD
    analyserRef.current = analyser;

    const src = ctx.createMediaStreamSource(stream);
    sourceRef.current = src;
    src.connect(analyser);

    startVadLoop();
  };

  const teardownAudioChain = () => {
    stopVadLoop();
    sourceRef.current?.disconnect();
    analyserRef.current?.disconnect();

    mediaStreamRef.current?.getTracks().forEach((t) => t.stop());
    mediaStreamRef.current = null;

    audioCtxRef.current?.close();
    audioCtxRef.current = null;

    setSpeaking(false);
    speechStartedRef.current = null;
    setElapsed(0);
  };

  // ====== ElevenLabs Start/Stop ======
  const handleStart = async () => {
    setError(null);
    setStatus("requesting-mic");

    try {
      await setupAudioChain();
      setStatus("connecting");

      // Lazy import to keep bundle light
      const { Conversation } = await import("@elevenlabs/client");

      // If your agent is PRIVATE, replace this with a fetch to your backend
      // to retrieve a conversation token (WebRTC) or signed URL (WebSocket).
      const session = await Conversation.startSession({
        agentId,
        connectionType: "webrtc", // or 'websocket' if you prefer manual audio handling
        // If you have a conversation token from your server:
        // token: await fetch('/api/elevenlabs/conversation-token').then(r => r.text())
      });

      conversationRef.current = session;
      setConnected(true);
      setStatus("connected");

      // Optionally, you can listen to events if exposed by the SDK
      // (depends on SDK version). Pseudo:
      // session.on('agent_speaking_start', () => { /* UI hint */ });
      // session.on('agent_speaking_end', () => { /* UI hint */ });

    } catch (e: any) {
      console.error(e);
      setError(e?.message || String(e));
      setStatus("error");
      teardownAudioChain();
    }
  };

  const handleStop = async () => {
    try {
      setStatus("stopping");
      const session = conversationRef.current;
      if (session) {
        try { await session.stopSession?.(); } catch {}
        conversationRef.current = null;
      }
    } finally {
      teardownAudioChain();
      setConnected(false);
      setStatus("idle");
    }
  };

  // ========== UI ==========
  return (
    <div className="w-full max-w-md mx-auto p-4">
      <div className="rounded-3xl border border-gray-200 shadow-sm p-5 bg-white">
        <div className="flex items-center justify-between mb-4">
          <h1 className="text-lg font-semibold tracking-tight">Korean Grammar Coach</h1>
          <span className={`text-xs px-2 py-1 rounded-full ${
              connected ? "bg-emerald-50 text-emerald-700" :
              status === "connecting" || status === "requesting-mic" ? "bg-amber-50 text-amber-700" :
              status === "error" ? "bg-rose-50 text-rose-700" : "bg-gray-50 text-gray-600"}
            `}>
            {status}
          </span>
        </div>

        <div className="flex flex-col items-center gap-3 py-6">
          {/* Timer */}
          <div className="text-4xl tabular-nums font-mono">
            {fmt(elapsed)}
          </div>
          <div className="text-xs text-gray-500">
            {speaking ? "listening… timer running" : "timer will start when you speak"}
          </div>

          {/* Controls */}
          <div className="flex gap-3 mt-4">
            {!connected ? (
              <button
                onClick={handleStart}
                className="px-4 py-2 rounded-2xl border border-gray-300 hover:bg-gray-50 active:scale-[.99]"
              >
                Start
              </button>
            ) : (
              <button
                onClick={handleStop}
                className="px-4 py-2 rounded-2xl border border-gray-300 hover:bg-gray-50 active:scale-[.99]"
              >
                Stop
              </button>
            )}
          </div>

          {/* Mic visualizer (simple dot that grows with RMS) */}
          <MicDot analyser={analyserRef.current} />
        </div>

        {error && (
          <div className="text-sm text-rose-600 border-t pt-4">
            {error}
          </div>
        )}

        <div className="mt-4 text-[11px] text-gray-500 leading-relaxed">
          Tip: keep your API key on the server. This demo uses WebRTC with a PUBLIC agent.
          If your agent is PRIVATE, create an API route that returns a conversation token
          from ElevenLabs and pass it to startSession.
        </div>
      </div>

      <div className="text-[11px] text-gray-400 mt-3 text-center">
        Built for @askcalvinoppa · Minimal container · Timer starts on speech
      </div>
    </div>
  );
}

function MicDot({ analyser }: { analyser: AnalyserNode | null }) {
  const [r, setR] = useState(6);
  useEffect(() => {
    if (!analyser) return;
    let raf: number;
    const data = new Uint8Array(analyser.fftSize);
    const loop = () => {
      analyser.getByteTimeDomainData(data);
      let sum = 0;
      for (let i = 0; i < data.length; i++) {
        const centered = data[i] - 128;
        sum += centered * centered;
      }
      const rms = Math.sqrt(sum / data.length); // ~0..128
      const radius = 6 + Math.min(18, rms / 3);
      setR(radius);
      raf = requestAnimationFrame(loop);
    };
    raf = requestAnimationFrame(loop);
    return () => cancelAnimationFrame(raf);
  }, [analyser]);

  return (
    <div className="mt-2 h-8 flex items-center justify-center">
      <div
        aria-hidden
        className="rounded-full"
        style={{ width: r * 2, height: r * 2, background: "#111", opacity: 0.8 }}
      />
    </div>
  );
}
